{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[NLP복습] transformer와 BERT, GPT .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN/xpcBa4Vj6p8Cxgz1MAlS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzhenxi/TIL/blob/main/%5BNLP%EB%B3%B5%EC%8A%B5%5D_transformer%EC%99%80_BERT%2C_GPT_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrJuGuGAxpne"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "왜 transformer가 중요한가? \n",
        "\n",
        "GPT : transformer의 디코더 아키텍쳐 활용\n",
        "\n",
        "BERT : transformer의 인코더 아키텍쳐 활용 \n",
        "\n",
        "트렌스포머의 등장 이후.. 이제 rnn은 사용하지 않는 추세....!\n",
        "\n",
        "attention 등장 이후로 입력 시쿼스 전체에서 정보를 추출하는 방향으로 발전하였다. \n",
        "\n",
        "seq2seq 모델의 한계점\n",
        "\n",
        "- 고정된 크기의 context vector (문맥 벡터)가 소스 문장의 모든 정보를 가지고 있어야 하므로 성능이 저하\n",
        "\n",
        "seq2sq with attention \n",
        "\n",
        "transformer - attention 만 있어도 다양한 task를 수행 할 수 있다! \n",
        "\n",
        "- rnn과 cnn을 전혀 사용하지 않음"
      ],
      "metadata": {
        "id": "Bb_RJtW-yErJ"
      }
    }
  ]
}